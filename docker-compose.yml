services:
  caddy:
    image: caddy:2.10
    restart: unless-stopped
    ports:
      - 127.0.0.1:8080:80
      - 127.0.0.1:3000:3000
      - 127.0.0.1:3001:3001
      - 127.0.0.1:3002:3002
      - 127.0.0.1:3003:3004
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - caddy
    environment:
      - FQDN=$FQDN
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY

  litellm:
    image: ghcr.io/berriai/litellm:v1.75.8-stable
    restart: unless-stopped
    volumes:
      - ./litellm_config.yaml:/app/litellm_config.yaml
      - $GOOGLE_CREDENTIAL_PATH:/app/google_credentials.json
    networks:
      - caddy
      - postgres
      - ollama
    command: ["--config", "/app/litellm_config.yaml", "--port", "4000", "--num_workers", "4"]
    environment:
      - ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
      - GOOGLE_APPLICATION_CREDENTIALS=/app/google_credentials.json
      - GOOGLE_VERTEX_LOCATION=$GOOGLE_VERTEX_LOCATION
      - GOOGLE_VERTEX_PROJECT=$GOOGLE_VERTEX_PROJECT
      - GROQ_API_KEY=$GROQ_API_KEY
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - POSTGRES_DB_URL=$POSTGRES_DB_URL
      - LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY
      - UI_PASSWORD=$LITELLM_UI_PASSWORD
      - UI_USERNAME=$LITELLM_UI_USERNAME

  postgres:
    image: postgres:16.9
    restart: unless-stopped
    shm_size: 128mb
    volumes:
      - postgres_db:/var/lib/postgresql/data
    networks:
      - postgres
    environment:
      - POSTGRES_USER=$POSTGRES_USER
      - POSTGRES_PASSWORD=$POSTGRES_PASSWORD

  ollama:
    image: ollama/ollama:0.11.8
    restart: unless-stopped
    volumes:
      - ollama:/root/.ollama
      - ./models:/models
    ports:
      - 127.0.0.1:11434:11434
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - ollama
    profiles:
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  anythingllm:
    image: mintplexlabs/anythingllm:1.8.5
    restart: unless-stopped
    init: true
    volumes:
      - anythingllm:/app/server/storage
      - ./.anythingllm.env:/app/server/.env
    networks:
      - caddy
    environment:
      - DISABLE_TELEMETRY=true
      - DISABLE_VIEW_CHAT_HISTORY=true
      - EMBEDDING_ENGINE=native
      - LITE_LLM_API_KEY=$LITELLM_MASTER_KEY
      - LITE_LLM_BASE_PATH=http://litellm:4000
      - LITE_LLM_MODEL_PREF=openai-gpt-4o
      - LITE_LLM_MODEL_TOKEN_LIMIT=16384
      - LLM_PROVIDER=litellm
      - SIMPLE_SSO_ENABLED=true
      - STORAGE_DIR=/app/server/storage
      - VECTOR_DB=lancedb

  anythingllm-auth:
    build: anythingllm-auth
    restart: unless-stopped
    init: true
    networks:
      - caddy
    environment:
      - ANYTHINGLLM_API_TOKEN=$ANYTHINGLLM_API_TOKEN

  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.26
    restart: unless-stopped
    init: true
    volumes:
      - ollama-webui:/app/backend/data
    networks:
      - caddy
      - ollama
    environment:
      - DEFAULT_USER_ROLE=user
      - ENABLE_ADMIN_CHAT_ACCESS=false
      - ENABLE_ADMIN_EXPORT=false
      - ENABLE_FORWARD_USER_INFO_HEADERS=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=http://litellm:4000
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
      - WEBUI_AUTH_TRUSTED_EMAIL_HEADER=$WEBUI_AUTH_TRUSTED_EMAIL_HEADER
      - WEBUI_AUTH_TRUSTED_NAME_HEADER=$WEBUI_AUTH_TRUSTED_NAME_HEADER
      #- BYPASS_MODEL_ACCESS_CONTROL=true

  ollama-webui:
    image: ollama-webui:local
    restart: unless-stopped
    init: true
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434/api
      - OPENAI_API_KEY=$LITELLM_MASTER_KEY
    networks:
      - caddy
      - ollama
    profiles:
      - custom-ui

networks:
  caddy:
  postgres:
  ollama:
volumes:
  anythingllm:
  caddy_config:
  caddy_data:
  postgres_db:
  ollama-webui:
  ollama:
