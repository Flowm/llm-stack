services:
  caddy:
    image: caddy:2.7
    restart: unless-stopped
    ports:
      - 127.0.0.1:3000:80
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - caddy
    environment:
      - FQDN=$FQDN
      - OPENAI_API_KEY=$OPENAI_API_KEY

  litellm:
    image: ghcr.io/berriai/litellm:main-v1.68.0-stable
    restart: unless-stopped
    volumes:
      - ./litellm_config.yaml:/app/litellm_config.yaml
      - $GOOGLE_CREDENTIAL_PATH:/app/google_credentials.json
    networks:
      - caddy
      - postgres
      - ollama
    ports:
      - 127.0.0.1:4000:4000
    command: [ "--config", "/app/litellm_config.yaml", "--port", "4000", "--num_workers", "4" ]
    environment:
      - ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
      - GOOGLE_APPLICATION_CREDENTIALS=/app/google_credentials.json
      - GOOGLE_VERTEX_LOCATION=$GOOGLE_VERTEX_LOCATION
      - GOOGLE_VERTEX_PROJECT=$GOOGLE_VERTEX_PROJECT
      - GROQ_API_KEY=$GROQ_API_KEY
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - POSTGRES_DB_URL=$POSTGRES_DB_URL
      - UI_PASSWORD=$UI_PASSWORD
      - UI_USERNAME=$UI_USERNAME

  postgres:
    image: postgres:16.2
    restart: unless-stopped
    shm_size: 128mb
    volumes:
      - postgres_db:/var/lib/postgresql/data
    networks:
      - postgres
    environment:
      - POSTGRES_USER=$POSTGRES_USER
      - POSTGRES_PASSWORD=$POSTGRES_PASSWORD

  ollama:
    image: ollama/ollama:0.3.3
    restart: unless-stopped
    volumes:
      - ollama:/root/.ollama
      - ./models:/models
    ports:
      - 127.0.0.1:11434:11434
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - ollama
    profiles:
      - gpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  anything:
    image: mintplexlabs/anythingllm:1.8.0
    restart: unless-stopped
    volumes:
      - anythingllm-ui:/app/server/storage
    environment:
      - STORAGE_DIR=/app/server/storage
      - LLM_PROVIDER=litellm
      - LITE_LLM_MODEL_PREF=openai-gpt-4o
      - LITE_LLM_MODEL_TOKEN_LIMIT=4096
      - LITE_LLM_BASE_PATH=http://litellm:4000
      - EMBEDDING_ENGINE=native
      - VECTOR_DB=lancedb
    networks: 
      - caddy

  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.6
    restart: unless-stopped
    init: true
    volumes:
      - ollama-webui:/app/backend/data
    networks:
      - caddy
    environment:
      - OPENAI_API_BASE_URL=http://litellm:4000
      - OPENAI_API_KEY=sk-openwebui

  nextchat:
    image: ${NEXTCHAT_IMAGE:-yidadaa/chatgpt-next-web:v2.16.0}
    restart: unless-stopped
    networks:
      - caddy
    environment:
      - OPENAI_API_KEY=any
      - BASE_URL=http://litellm:4000/
      - HIDE_USER_API_KEY=1
      - CUSTOM_MODELS=-all,openai-gpt-4o,openai-gpt-4-turbo,openai-o3,openai-o4,anthropic-claude-3-7-sonnet,anthropic-claude-3-5-sonnet

  ollama-webui:
    image: ollama-webui:local
    restart: unless-stopped
    init: true
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434/api
    networks:
      - caddy
      - ollama
    profiles:
      - custom-ui

networks:
  caddy:
  postgres:
  ollama:

volumes:
  anythingllm-ui:
  caddy_config:
  caddy_data:
  postgres_db:
  ollama-webui:
  ollama:
